{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitvenvvirtualenv6b4f093d3fee42a4b8c0f6bd7f8294ac",
   "display_name": "Python 3.7.4 64-bit ('.venv': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Overwriting glue-etl.py\n"
    }
   ],
   "source": [
    "%%writefile glue-etl.py\n",
    "# to be used in Glue jobs only\n",
    "import sys\n",
    "from awsglue.utils import getResolvedOptions\n",
    "args = getResolvedOptions(sys.argv,['JOB_NAME','S3_BUCKET','deal_data_key','price_data_key'])\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook only\n",
    "args={\n",
    "    'S3_BUCKET':'bucket-mldemo-test-20200330083758476200000003',\n",
    "    'deal_data_key':'deals-1585662111673.csv',\n",
    "    'price_data_key':'price-1585662111673.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Appending to glue-etl.py\n"
    }
   ],
   "source": [
    "%%writefile -a glue-etl.py\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import math\n",
    "import io\n",
    "import logging,os\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker.amazon.common as smac\n",
    "import boto3\n",
    "from s3fs.core import S3FileSystem\n",
    "\n",
    "s3fs = S3FileSystem(anon=False)\n",
    "session=boto3.Session()\n",
    "\n",
    "S3_BUCKET=args['S3_BUCKET']\n",
    "deal_data_path='s3://%s/staging/%s' % (S3_BUCKET,args['deal_data_key'])\n",
    "price_data_path='s3://%s/staging/%s' % (S3_BUCKET,args['price_data_key'])\n",
    "\n",
    "def initialise_logger():\n",
    "    logging.basicConfig(level=os.environ.get(\"LOGLEVEL\", \"WARN\"),\n",
    "                        format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',\n",
    "                        datefmt='%m-%d %H:%M')\n",
    "\n",
    "    logger=logging.getLogger('data-processor')\n",
    "    return logger\n",
    "\n",
    "def get_deal_data(deal_data_path):\n",
    "    logger.warn('Getting deals from CSV')\n",
    "\n",
    "    deals=pd.read_csv(s3fs.open(deal_data_path),\n",
    "        header=None,\n",
    "        names=[\"timestamp\",\"loss_or_profit\",\"type\",\"stop\",\"limit\",\"price\",\"stop_price\",\"limit_price\"],\n",
    "        dtype={\n",
    "            \"timestamp\":str\n",
    "        }\n",
    "    )\n",
    "\n",
    "    query='type==\"buy\"'\n",
    "    logger.warn('Getting qualified deals - %s' % query)\n",
    "    qualified_deals=deals.query(query)\n",
    "\n",
    "    return qualified_deals\n",
    "\n",
    "def get_price_data(price_data_path):\n",
    "    logger.warn('Getting price from CSV')\n",
    "    price=pd.read_csv(s3fs.open(price_data_path),\n",
    "        header=None,\n",
    "        names=[\"timestamp\",\"BID\",\"OFFER\",\"MID_OPEN\",\"CHANGE\",\"CHANGE_PCT\",\"HIGH\",\"LOW\",\"UPDATE_TIME\",\"MARKET_STATE\",\"MARKET_DELAY\"],\n",
    "        dtype={\n",
    "            \"timestamp\":str,\n",
    "            \"datetime\":str\n",
    "        }\n",
    "    )\n",
    "    return price\n",
    "\n",
    "def join_deals_with_price(qualified_deals,price):\n",
    "    logger.warn('Joining deals with price')\n",
    "    deals_with_price=pd.merge(qualified_deals, price,how=\"left\",on=\"timestamp\",sort=True)\n",
    "\n",
    "    # Calculating MID price, the average of BID and OFFER\n",
    "    deals_with_price['MID']=(deals_with_price['BID']+deals_with_price['OFFER'])/2\n",
    "\n",
    "     # deduplicate\n",
    "\n",
    "    duplicated=deals_with_price[deals_with_price.duplicated(subset='timestamp',keep='first')]\n",
    "\n",
    "    deals_with_price.drop(duplicated.index,inplace=True)\n",
    "    \n",
    "    deals_with_price.reset_index(inplace=True)\n",
    "\n",
    "    return deals_with_price\n",
    "\n",
    "def calculate_mean_price_each_second(price):\n",
    "    logger.warn('Calculating mean price for each second')\n",
    "    price['timestamp_second']=round(price['timestamp'].astype(float)/1000)\n",
    "    mean_price_each_second=price[['timestamp_second','BID','OFFER']].groupby('timestamp_second').mean().reset_index()\n",
    "    mean_price_each_second['MID']=(mean_price_each_second['BID']+mean_price_each_second['OFFER'])/2\n",
    "    return mean_price_each_second\n",
    "\n",
    "def get_historical_price(deals_with_price,mean_price_each_second):\n",
    "    logger.warn('Getting historical price for each deal')\n",
    "\n",
    "    deals_with_price['timestamp_second']=round(deals_with_price['timestamp'].astype(float)/1000)\n",
    "\n",
    "    df=pd.DataFrame()\n",
    "    historical_data_points=600\n",
    "    for i,row in deals_with_price.iterrows():\n",
    "        values=np.full(historical_data_points,np.nan)\n",
    "        _timestamp=row['timestamp_second']\n",
    "        prices=mean_price_each_second[['timestamp_second','MID']].query('timestamp_second<%s' % _timestamp).tail(historical_data_points)['MID']\n",
    "        values[0:len(prices)]=prices.values\n",
    "        value_series=pd.Series(values)\n",
    "        df.insert(loc=i,column=str(row['timestamp']),value=values)\n",
    "\n",
    "    all_data_absolute=deals_with_price[['timestamp','BID','OFFER','MID_OPEN','HIGH','LOW']].merge(df.transpose(),left_on='timestamp',right_index=True)\n",
    "    all_data_absolute.drop('timestamp',axis=1,inplace=True)\n",
    "\n",
    "    logger.warn('Filling missing data')\n",
    "\n",
    "    all_data_absolute.fillna(method='backfill',inplace=True)\n",
    "    all_data_absolute.fillna(method='pad',inplace=True)\n",
    "\n",
    "    return all_data_absolute\n",
    "\n",
    "def calculate_relative_price(all_data_absolute):\n",
    "    logger.warn('Calculating the price relative to MID for each deal')\n",
    "    mid=(all_data_absolute['BID']+all_data_absolute['OFFER'])/2\n",
    "    all_data=all_data_absolute.subtract(mid,axis='index')\n",
    "    return all_data\n",
    "\n",
    "def get_labels(deals_with_price):\n",
    "    logger.warn('Getting labels for each deal')\n",
    "    labels=deals_with_price['loss_or_profit'].apply(lambda x:1 if x=='profit' else 0)\n",
    "    return labels\n",
    "\n",
    "def create_datasets(all_data,labels):\n",
    "    logger.warn('Creating datasets')\n",
    "    count=len(all_data)\n",
    "\n",
    "    training=math.floor(0.6*count)\n",
    "    validation=math.floor(0.2*count)\n",
    "    test=math.floor(0.2*count)\n",
    "\n",
    "    datasets={\n",
    "        'training':{},\n",
    "        'validation':{},\n",
    "        'test':{}\n",
    "    }\n",
    "\n",
    "    datasets['training']['data']=all_data[0:training]\n",
    "    datasets['training']['labels']=labels[0:training]\n",
    "\n",
    "    logger.warn('Training dataset created')\n",
    "\n",
    "    datasets['validation']['data']=all_data[(training+1):(training+validation)]\n",
    "    datasets['validation']['labels']=labels[(training+1):(training+validation)]\n",
    "\n",
    "    logger.warn('Validation dataset created')\n",
    "\n",
    "    datasets['test']['data']=all_data[(training+validation+1):]\n",
    "    datasets['test']['labels']=labels[(training+validation+1):]\n",
    "\n",
    "    logger.warn('Test dataset created')\n",
    "\n",
    "    logger.warn(\"training data length: %s, training label length: %s\" %(len(datasets['training']['data']),len(datasets['training']['labels'])))\n",
    "    logger.warn(\"validation data length: %s, validation label length: %s\" %(len(datasets['validation']['data']),len(datasets['validation']['labels'])))\n",
    "    logger.warn(\"test data length: %s, test label length: %s\" %(len(datasets['test']['data']),len(datasets['test']['labels'])))\n",
    "\n",
    "    return datasets\n",
    "\n",
    "def save_dataset(dataset,key):\n",
    "    data_np=dataset['data'].to_numpy().astype('float32')\n",
    "    labels_np=dataset['labels'].to_numpy().astype('float32')\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_numpy_to_dense_tensor(buf, data_np, labels_np)\n",
    "    buf.seek(0)\n",
    "    session.resource('s3').Bucket(S3_BUCKET).Object('transformed/%s_data.io' % key).upload_fileobj(buf)\n",
    "    logger.warn('%s dataset saved to S3' % key)\n",
    "\n",
    "def save_datasets(datasets):\n",
    "    logger.warn('Saving datasets')\n",
    "    for key in datasets:\n",
    "        save_dataset(datasets[key],key)\n",
    "\n",
    "def save_all_data_as_csv(all_data):\n",
    "    logger.warn('Saving all data as CSV for verification')\n",
    "    csv_data=all_data.copy()\n",
    "    csv_data['profit']=labels\n",
    "\n",
    "    csv_data.to_csv('all_data.csv',header=None,index=False)\n",
    "\n",
    "    s3 = session.client('s3')\n",
    "    with open('all_data.csv', \"rb\") as f:\n",
    "        s3.upload_fileobj(f, S3_BUCKET,'transformed/all_data.csv')\n",
    "        logger.warn('CSV saved to S3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only in Jupyter notebook\n",
    "session=boto3.Session(profile_name='ml-lab')\n",
    "s3fs = S3FileSystem(anon=False, session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Appending to glue-etl.py\n"
    }
   ],
   "source": [
    "%%writefile -a glue-etl.py\n",
    "\n",
    "logger=initialise_logger()\n",
    "\n",
    "logger.warn('Starting data processing')\n",
    "\n",
    "qualified_deals=get_deal_data(deal_data_path)\n",
    "\n",
    "price=get_price_data(price_data_path)\n",
    "\n",
    "deals_with_price=join_deals_with_price(qualified_deals,price)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Appending to glue-etl.py\n"
    }
   ],
   "source": [
    "%%writefile -a glue-etl.py\n",
    "\n",
    "mean_price_each_second=calculate_mean_price_each_second(price)\n",
    "\n",
    "all_data_absolute=get_historical_price(deals_with_price,mean_price_each_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Appending to glue-etl.py\n"
    }
   ],
   "source": [
    "%%writefile -a glue-etl.py\n",
    "\n",
    "all_data=calculate_relative_price(all_data_absolute)\n",
    "\n",
    "labels=get_labels(deals_with_price)\n",
    "\n",
    "datasets=create_datasets(all_data,labels)\n",
    "\n",
    "save_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Appending to glue-etl.py\n"
    }
   ],
   "source": [
    "%%writefile -a glue-etl.py\n",
    "\n",
    "save_all_data_as_csv(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}